# LLM Integration: Intelligent API Spec Detection System

## Current Problem Analysis
Replace the expensive 42+ path pattern-matching system with LLM-based content analysis to:
- ✅ Reduce GitHub API usage by 80%+
- ✅ Eliminate complex path pattern maintenance
- ✅ Improve detection accuracy from ~70% to 90%+
- ✅ Future-proof against new spec formats
- ✅ Reduce scanning time from minutes to seconds

## Implementation Requirements

### 1. LLM Service Setup
Add LLM integration with cost controls and fallback mechanisms:

```typescript
// Create: server/services/llmSpecDetector.ts
interface LLMSpecAnalysis {
  isApiSpec: boolean;
  specType: 'openapi-3.x' | 'swagger-2.x' | 'asyncapi' | 'graphql' | 'unknown';
  confidence: number; // 1-10
  reasoning: string;
  endpoints?: number; // estimated endpoint count
}

class LLMSpecDetector {
  private readonly maxContentLength = 3000; // Cost control
  private readonly confidenceThreshold = 7;
  
  async analyzeFile(filePath: string, content: string): Promise<LLMSpecAnalysis>;
  async batchAnalyzeFiles(files: Array<{path: string, content: string}>): Promise<LLMSpecAnalysis[]>;
}
```

### 2. Replace Current Detection Logic
**Target Files to Modify:**
- `server/services/repositoryScanner.ts` - Replace scanRepositoryForSpecs()
- `server/services/spec-utils.ts` - Add LLM detection methods

**Current Code to Replace:**
```typescript
// REMOVE: Complex path pattern checking
const potentialSpecFiles = data.tree.filter(item => 
  item.type === 'blob' && 
  item.path && 
  isLikelySpecFile(item.path) // This entire function
);
```

**New Implementation:**
```typescript
// NEW: Smart two-stage detection
async scanRepositoryForSpecs(owner: string, repo: string, ref = 'HEAD'): Promise<SpecInfo[]> {
  // Stage 1: Fast file filtering (YAML/JSON only)
  const candidateFiles = await this.getCandidateFiles(owner, repo, ref);
  
  // Stage 2: LLM content analysis (batch process)
  const llmAnalysis = await this.llmDetector.batchAnalyzeFiles(candidateFiles);
  
  // Stage 3: Process confirmed specs
  return this.processConfirmedSpecs(llmAnalysis.filter(a => a.confidence >= 7));
}
```

### 3. Environment Configuration
Add these environment variables:
```bash
# LLM Configuration
LLM_PROVIDER=openai  # or anthropic, google, etc.
LLM_API_KEY=your_api_key_here
LLM_MODEL=gpt-4o-mini  # Cost-effective model
LLM_MAX_TOKENS=1000
LLM_TIMEOUT=10000  # 10 second timeout

# Cost Controls
LLM_DAILY_BUDGET=10.00  # $10/day limit
LLM_CACHE_TTL=86400     # 24 hour cache
LLM_ENABLED=true        # Feature flag
```

### 4. Cost Control Implementation
```typescript
class LLMCostController {
  private dailyUsage = 0;
  private cache = new Map<string, LLMSpecAnalysis>();
  
  async checkBudget(): Promise<boolean>;
  async trackUsage(cost: number): Promise<void>;
  async getCachedResult(fileHash: string): Promise<LLMSpecAnalysis | null>;
  async setCachedResult(fileHash: string, result: LLMSpecAnalysis): Promise<void>;
}
```

### 5. Fallback Strategy
```typescript
async detectSpecsWithFallback(owner: string, repo: string): Promise<SpecInfo[]> {
  try {
    // Try LLM detection first
    if (process.env.LLM_ENABLED === 'true') {
      return await this.llmBasedDetection(owner, repo);
    }
  } catch (error) {
    logger.warn('LLM detection failed, falling back to pattern matching', { error });
  }
  
  // Fallback to current system
  return await this.patternBasedDetection(owner, repo);
}
```

### 6. Performance Optimization
```typescript
// Batch processing for efficiency
async batchAnalyzeRepository(owner: string, repo: string): Promise<SpecInfo[]> {
  const candidateFiles = await this.getYamlJsonFiles(owner, repo);
  
  // Process in batches of 10 to control API usage
  const batchSize = 10;
  const batches = this.chunkArray(candidateFiles, batchSize);
  
  const results = [];
  for (const batch of batches) {
    const batchResults = await Promise.all(
      batch.map(file => this.analyzeSingleFile(file))
    );
    results.push(...batchResults);
    
    // Rate limiting between batches
    await this.sleep(1000);
  }
  
  return results.filter(r => r.confidence >= this.confidenceThreshold);
}
```

### 7. LLM Prompt Engineering
```typescript
private buildAnalysisPrompt(filePath: string, content: string): string {
  return `Analyze this file to determine if it contains API specifications.

FILEPATH: ${filePath}
CONTENT (first 3000 chars):
${content.substring(0, 3000)}

Return ONLY valid JSON with this exact structure:
{
  "isApiSpec": boolean,
  "specType": "openapi-3.x" | "swagger-2.x" | "asyncapi" | "graphql" | "unknown",
  "confidence": number (1-10),
  "reasoning": "brief explanation",
  "endpoints": number (estimated count of endpoints/operations)
}

CRITERIA:
- Look for: openapi, swagger, paths, operations, endpoints, schemas, definitions
- Ignore: configuration files, documentation, examples
- Consider: file structure, keywords, content patterns
- Confidence 8+: Definitely an API spec
- Confidence 5-7: Possibly an API spec  
- Confidence <5: Probably not an API spec

RESPOND WITH ONLY THE JSON OBJECT.`;
}
```

### 8. Integration Points
**Modify these existing functions:**

```typescript
// In repositoryScanner.ts
- Replace: isLikelySpecFile() 
+ Add: llmBasedSpecDetection()

// In spec-utils.ts  
- Replace: detectSpecFormat()
+ Add: llmEnhancedFormatDetection()

// In routes.ts
- Update: /api/projects/:id/scan endpoint
+ Add: LLM detection status and confidence scores
```

### 9. Monitoring and Analytics
```typescript
interface LLMDetectionMetrics {
  totalFilesAnalyzed: number;
  specsDetected: number;
  averageConfidence: number;
  costPerDetection: number;
  processingTime: number;
  fallbackUsage: number;
}

// Track improvement over pattern matching
class DetectionAnalytics {
  async compareDetectionMethods(repo: string): Promise<ComparisonReport>;
  async trackDailyMetrics(): Promise<LLMDetectionMetrics>;
}
```

### 10. Testing Strategy
```typescript
// Create test cases with known repositories
const testRepositories = [
  'swagger-api/swagger-petstore',    // Clear OpenAPI
  'asyncapi/asyncapi',              // AsyncAPI specs  
  'microsoft/vscode',               // Mixed content
  'facebook/react'                  // No specs
];

// Measure improvement
interface TestResults {
  patternMatchingResults: SpecInfo[];
  llmDetectionResults: SpecInfo[];
  improvementMetrics: {
    additionalSpecsFound: number;
    falsePositivesReduced: number;
    processingTimeComparison: number;
  };
}
```

## Success Metrics
- **Detection Accuracy:** >90% (vs current ~70%)
- **GitHub API Calls:** Reduce by 80%
- **Processing Time:** <30 seconds per repository  
- **False Positive Rate:** <10%
- **Monthly LLM Cost:** <$50 for typical usage
- **Maintenance Overhead:** Eliminate pattern file updates

## Implementation Timeline
1. **Week 1:** LLM service setup and basic detection
2. **Week 2:** Batch processing and cost controls  
3. **Week 3:** Integration with existing scanner
4. **Week 4:** Testing, fallback mechanisms, and optimization

## Rollout Strategy
1. **Phase 1:** A/B test on 10% of repositories
2. **Phase 2:** Compare results vs pattern matching
3. **Phase 3:** Full rollout with pattern matching fallback
4. **Phase 4:** Remove old pattern matching system

Implement this LLM-based spec detection system to solve the immediate operational issues with the current 42+ path system while improving accuracy and reducing maintenance overhead.